{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 머신러닝 분류\n",
    "\n",
    "#### [1] 지도학습(Supervised Learning) : 답이 주어진 상태에서 학습\n",
    "* 회귀(Regression)\n",
    "* 분류(Classification)\n",
    "\n",
    "#### [2] 비지도학습(Unsupervised Learning) : 답이 주어지지 않고 학습\n",
    "* 군집화(Clustering)\n",
    "* 차원 축소(Dimension Reduction) : PCA(주성분 분석, Pricipal Component Analysis)\n",
    "\n",
    "#### [3] 강화 학습(Reinforcement Learning) : 답을 모르고 있는 상태에서 답을 알아가는 강한 인공지능(자아를 갖음, 인간수준), 게임, 알파고(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론과 XOR Problem\n",
    "- 퍼셉트론 : 다수의 신호(흐름이 있는)를 입력으로 받아 하나의 신호를 출력한다\n",
    "- XOR Problem : 단층 퍼셉트론으로 XOR 출력을 얻도록 학습 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def AND(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.5\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta :  # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "\n",
    "print(AND(0,0))    \n",
    "print(AND(0,1))\n",
    "print(AND(1,0))\n",
    "print(AND(1,1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def NAND(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.5\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta :  # 임계값\n",
    "        return 1\n",
    "    elif tmp > theta:\n",
    "        return 0\n",
    "\n",
    "print(NAND(0,0))    \n",
    "print(NAND(0,1))\n",
    "print(NAND(1,0))\n",
    "print(NAND(1,1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def OR(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.4\n",
    "    tmp = w1*x1 + w2*x2\n",
    "    if tmp <= theta :  # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "\n",
    "print(OR(0,0))    \n",
    "print(OR(0,1))\n",
    "print(OR(1,0))\n",
    "print(OR(1,1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단층 퍼셉트론의 한계\n",
    "# XOR Problem : 아무리 학습시켜도 weight을 구할 수 가 없음\n",
    "# def XOR(x1,x2):\n",
    "#     w1,w2,theta = _,_,_\n",
    "#     tmp = w1*x1 + w2*x2\n",
    "#     if tmp <= theta :  # 임계값\n",
    "#         return 0\n",
    "#     elif tmp > theta:\n",
    "#         return 1\n",
    "\n",
    "# print(XOR(0,0))    # 0 \n",
    "# print(XOR(0,1))    # 1\n",
    "# print(XOR(1,0))    # 1\n",
    "# print(XOR(1,1))    # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem 조건 --> 선형으로 해결  , abs()/sin()/exp()\n",
    "def XOR(x1,x2):\n",
    "    w1,w2,theta = 0.5,-0.5,0.4\n",
    "    tmp = abs(w1*x1 + w2*x2)\n",
    "    if tmp <= theta :  # 임계값\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1\n",
    "\n",
    "print(XOR(0,0))    # 0 \n",
    "print(XOR(0,1))    # 1\n",
    "print(XOR(1,0))    # 1\n",
    "print(XOR(1,1))    # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation : 1986년 제프리 힌튼(Geoffrey Hinton)\n",
    "샘플에 대한 신경망의 오차를 다시 출력층에서부터 입력층으로 거꾸로 전파시켜 각 층의 가중치(weight)를 계산하는 방법. 이를 통해 weight와 bias를 알맞게 학습할 수 있다¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층퍼셉트론(MLP)으로 XOR Problem 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem : 서로 다른 weight을 갖는 다층신경망을 사용하여 해결\n",
    "def XOR(x1,x2):\n",
    "    s1 = NAND(x1,x2)\n",
    "    s2 = OR(x1,x2)\n",
    "    y = AND(s1,s2)\n",
    "    return y\n",
    "\n",
    "print(XOR(0,0))    # 0 \n",
    "print(XOR(0,1))    # 1\n",
    "print(XOR(1,0))    # 1\n",
    "print(XOR(1,1))    # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀(Regression)모델\n",
    "#### [1] 선형 회귀(Linear Regression) : 1차 함수, 직선의 방정식\n",
    "#### [2] 가중치(Weight) : 입력변수가 출력에 영향을 미치는 정도를 설정, 기울기 값 , 회귀 계수\n",
    "#### [3] 편향(Bias) : 기본 출력 값이 활성화 되는 정도를 설정, y 절편, 회귀 계수\n",
    "#### [4] 비용함수(Cost Function) : 2차 함수, 포물선의 방정식, (예측값 - 실제값)^2\n",
    "* cost(비용) = 오차 = 에러 = 손실(loss)\n",
    "* cost(W,b) = (H(x) - y)^2\n",
    "\n",
    "#### [5] 예측(가설,Hypothesis)함수: predict, H(x) : 예측값, y값:답,결정값,target,label, x:입력, 피쳐(feature)\n",
    "* H(X) = W*X + b\n",
    "\n",
    "#### [6] 경사 하강법(Gradient Descent Algorithm)\n",
    "#### : 비용(cost) 이 가장 작은 Weight(가중치) 값을 구하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:-1,cost: 18.666666666666668\n",
      "w:0 ,cost: 4.666666666666667\n",
      "w:1 ,cost: 0.0\n",
      "w:2 ,cost: 4.666666666666667\n",
      "w:3 ,cost: 18.666666666666668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 비용 함수의 구현\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cost(x,y,w) :\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]          # 예측 함수(방정식)\n",
    "        loss = (hx - y[k])**2  # (예측값 - 실제값)^2\n",
    "        c += loss\n",
    "    return c/len(x)            #  평균 제곱 오차\n",
    "\n",
    "x = [1,2,3]\n",
    "y = [1,2,3]\n",
    "\n",
    "print('w:-1,cost:',cost(x,y,-1))   # hx = [-1,-2,-3] , cost: 18.666666666666668\n",
    "print('w:0 ,cost:',cost(x,y,0))    # hx = [0,0,0]\n",
    "print('w:1 ,cost:',cost(x,y,1))    # hx = [1,2,3]    , w=1, cost = 0.0 , 최저점\n",
    "print('w:2 ,cost:',cost(x,y,2))    # hx = [2,4,6]\n",
    "print('w:3 ,cost:',cost(x,y,3))    # hx = [1,2,3]  ,  cost: 18.666666666666668\n",
    "\n",
    "# 비용함수의 시각화 : x축은 weight, y축은 cost로 하는 2차 함수, 포물선의 방정식\n",
    "for k in range(-30,50):\n",
    "    w = k/10\n",
    "    c = cost(x,y,w)\n",
    "    plt.plot(w,c,'ro') # 'r':red , 'o': 점으로 출력\n",
    "    \n",
    "plt.title(' ** Cost Function Graph')    \n",
    "plt.xlabel('weight')    \n",
    "plt.ylabel('cost')\n",
    "plt.grid()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미분 : 순간 변화량, 기울기, x축으로 1만큼 움직였을 때 y축으로 움직인 거리\n",
    "#### 함수의 미분 공식 정리 :  f(x) = x^n   ====> f'(x) = n*x^(n-1)\n",
    "* y = 3            ===> y' = 0\n",
    "* y = 2*x          ===> y' = 2\n",
    "* y = x^2          ===> y' = 2*x\n",
    "* y = (x + 1)^2    ===> y' = 2**(x + 1) # y = x^2 + 2**x + 1  ===> 2**x + 2\n",
    "\n",
    "* 곱셈 공식 : (a + b)^2 = a^2 + 2**a**b + b^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- start learning!!\n",
      "[000] cost: 378.0 old: 100 weight: 5.8\n",
      "[001] cost: 107.51999999999998 old: 378.0 weight: 3.56\n",
      "[002] cost: 30.583466666666666 old: 107.51999999999998 weight: 2.365333333333333\n",
      "[003] cost: 8.69929718518518 old: 30.583466666666666 weight: 1.7281777777777778\n",
      "[004] cost: 2.47446675489712 old: 8.69929718518518 weight: 1.3883614814814815\n",
      "[005] cost: 0.7038483213929583 old: 2.47446675489712 weight: 1.2071261234567903\n",
      "[006] cost: 0.2002057447517751 old: 0.7038483213929583 weight: 1.1104672658436214\n",
      "[007] cost: 0.05694741184050483 old: 0.2002057447517751 weight: 1.0589158751165981\n",
      "[008] cost: 0.016198374923521403 old: 0.05694741184050483 weight: 1.0314218000621858\n",
      "[009] cost: 0.004607537756023892 old: 0.016198374923521403 weight: 1.0167582933664991\n",
      "[010] cost: 0.0013105885172690224 old: 0.004607537756023892 weight: 1.008937756462133\n",
      "[011] cost: 0.0003727896226898598 old: 0.0013105885172690224 weight: 1.004766803446471\n",
      "[012] cost: 0.0001060379371206724 old: 0.0003727896226898598 weight: 1.002542295171451\n",
      "[013] cost: 3.0161902114324568e-05 old: 0.0001060379371206724 weight: 1.0013558907581073\n",
      "[014] cost: 8.579385490296031e-06 old: 3.0161902114324568e-05 weight: 1.0007231417376572\n",
      "[015] cost: 2.4403585394623746e-06 old: 8.579385490296031e-06 weight: 1.000385675593417\n",
      "[016] cost: 6.941464290019953e-07 old: 2.4403585394623746e-06 weight: 1.0002056936498225\n",
      "[017] cost: 1.9744609536079287e-07 old: 6.941464290019953e-07 weight: 1.0001097032799053\n",
      "[018] cost: 5.616244490256599e-08 old: 1.9744609536079287e-07 weight: 1.0000585084159495\n",
      "[019] cost: 1.597509543893363e-08 old: 5.616244490256599e-08 weight: 1.0000312044885065\n",
      "[020] cost: 4.544027147100321e-09 old: 1.597509543893363e-08 weight: 1.0000166423938701\n",
      "[021] cost: 1.292523277397425e-09 old: 4.544027147100321e-09 weight: 1.0000088759433974\n",
      "[022] cost: 3.6765106557353845e-10 old: 1.292523277397425e-09 weight: 1.0000047338364786\n",
      "[023] cost: 1.0457630309680019e-10 old: 3.6765106557353845e-10 weight: 1.0000025247127886\n",
      "[024] cost: 2.974614843652282e-11 old: 1.0457630309680019e-10 weight: 1.0000013465134872\n",
      "[025] cost: 8.461126664488492e-12 old: 2.974614843652282e-11 weight: 1.0000007181405266\n",
      "[026] cost: 2.4067204744183466e-12 old: 8.461126664488492e-12 weight: 1.0000003830082809\n",
      "[027] cost: 6.845782681618235e-13 old: 2.4067204744183466e-12 weight: 1.0000002042710832\n",
      "[028] cost: 1.9472448527085553e-13 old: 6.845782681618235e-13 weight: 1.0000001089445776\n",
      "[029] cost: 5.538829787885448e-14 old: 1.9472448527085553e-13 weight: 1.0000000581037747\n",
      "[030] cost: 1.575489361658054e-14 old: 5.538829787885448e-14 weight: 1.0000000309886798\n",
      "[031] cost: 4.481391952569267e-15 old: 1.575489361658054e-14 weight: 1.000000016527296\n",
      "[032] cost: 1.2747070573345361e-15 old: 4.481391952569267e-15 weight: 1.0000000088145578\n",
      "[033] cost: 3.6258332831368247e-16 old: 1.2747070573345361e-15 weight: 1.0000000047010975\n",
      "[034] cost: 1.0313481853667948e-16 old: 3.6258332831368247e-16 weight: 1.000000002507252\n",
      "[035] cost: 2.933612341505031e-17 old: 1.0313481853667948e-16 weight: 1.000000001337201\n",
      "[036] cost: 8.344497274162171e-18 old: 2.933612341505031e-17 weight: 1.000000000713174\n",
      "[037] cost: 2.3735464825145457e-18 old: 8.344497274162171e-18 weight: 1.0000000003803595\n",
      "[038] cost: 6.751422044227725e-19 old: 2.3735464825145457e-18 weight: 1.0000000002028584\n",
      "[039] cost: 1.920405593646558e-19 old: 6.751422044227725e-19 weight: 1.0000000001081912\n",
      "[040] cost: 5.462493428133877e-20 old: 1.920405593646558e-19 weight: 1.000000000057702\n",
      "[041] cost: 1.5537773320496455e-20 old: 5.462493428133877e-20 weight: 1.0000000000307745\n",
      "[042] cost: 4.419657596231936e-21 old: 1.5537773320496455e-20 weight: 1.000000000016413\n",
      "[043] cost: 1.2571515848923574e-21 old: 4.419657596231936e-21 weight: 1.0000000000087537\n",
      "[044] cost: 3.5758710617043957e-22 old: 1.2571515848923574e-21 weight: 1.0000000000046687\n",
      "[045] cost: 1.0171864166944069e-22 old: 3.5758710617043957e-22 weight: 1.00000000000249\n",
      "[046] cost: 2.8933990559698716e-23 old: 1.0171864166944069e-22 weight: 1.000000000001328\n",
      "[047] cost: 8.231253103671774e-24 old: 2.8933990559698716e-23 weight: 1.0000000000007083\n",
      "[048] cost: 2.341362175139032e-24 old: 8.231253103671774e-24 weight: 1.0000000000003777\n",
      "[049] cost: 6.658942698258942e-25 old: 2.341362175139032e-24 weight: 1.0000000000002014\n",
      "[050] cost: 1.891893061517278e-25 old: 6.658942698258942e-25 weight: 1.0000000000001075\n",
      "[051] cost: 5.389865839559056e-26 old: 1.891893061517278e-25 weight: 1.0000000000000573\n",
      "[052] cost: 1.5315340044413334e-26 old: 5.389865839559056e-26 weight: 1.0000000000000306\n",
      "[053] cost: 4.38172789805011e-27 old: 1.5315340044413334e-26 weight: 1.0000000000000164\n",
      "[054] cost: 1.2599423424554927e-27 old: 4.38172789805011e-27 weight: 1.0000000000000087\n",
      "[055] cost: 3.46129156767911e-28 old: 1.2599423424554927e-27 weight: 1.0000000000000047\n",
      "[056] cost: 1.0355442841244991e-28 old: 3.46129156767911e-28 weight: 1.0000000000000024\n",
      "[057] cost: 2.677196697093809e-29 old: 1.0355442841244991e-28 weight: 1.0000000000000013\n",
      "[058] cost: 8.283039504820624e-30 old: 2.677196697093809e-29 weight: 1.0000000000000007\n",
      "[059] cost: 1.791371638939381e-30 old: 8.283039504820624e-30 weight: 1.0000000000000004\n",
      "[060] cost: 9.203377227578472e-31 old: 1.791371638939381e-30 weight: 1.0000000000000002\n",
      "[061] cost: 3.4512664603419266e-31 old: 9.203377227578472e-31 weight: 1.0\n",
      "[062] cost: 0.0 old: 3.4512664603419266e-31 weight: 1.0\n",
      "[063] cost: 0.0 old: 0.0 weight: 1.0\n",
      "----------- end learning!!\n",
      "weight: 1.0 train: 64 회\n"
     ]
    }
   ],
   "source": [
    "# 경사 하강법 알고리즘 함수 구현, 미분 적용\n",
    "def gradient_descent(x,y,w):\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w*x[k] \n",
    "        loss = (hx - y[k])*x[k]     # 곱하기 2를 생략한 비용함수의 미분\n",
    "        c += loss\n",
    "        # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "        # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "        # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k] = 2*x[k]*(w*x[k] - y[k])\n",
    "        # =  2*x[k]*(hx - y[k])\n",
    "    return c/len(x)\n",
    "\n",
    "# x = [1,2,3]\n",
    "# y = [1.2,3.2,5.4]\n",
    "\n",
    "# 학습 시작(train,fit) 시작\n",
    "print('----------- start learning!!')\n",
    "w , old = 10, 100\n",
    "for k in range(1000):\n",
    "    c = cost(x,y,w)\n",
    "    grad = gradient_descent(x,y,w)\n",
    "    w -= 0.1*grad  # 0.1:학습율(learning rate),하이퍼 파라메터,가중치의 업데이트실행\n",
    "    print('[%03d]'%k,'cost:',c,'old:',old,'weight:',w)\n",
    "#   if c == old:  # cost의 변화가 없을 때\n",
    "    if c >= old and abs(c - old) < 1.0e-15: # cost가 1.0e-15값 보다도 더 줄지 않을 때\n",
    "        break\n",
    "    old = c\n",
    "print('----------- end learning!!')\n",
    "print('weight:',w,'train:',k+1,'회')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# 알고리즘 구현 : 비용함수와 경사 하강법 알고리즘 함수 구현\n",
    "\n",
    "# (1) 비용함수 구현\n",
    "def cost(x,y,w) :\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w * x[k]          # 예측 함수(방정식)\n",
    "        loss = (hx - y[k])**2  # (예측값 - 실제값)^2\n",
    "        c += loss\n",
    "    return c/len(x)            #  평균 제곱 오차\n",
    "\n",
    "# (2) 경사 하강법 알고리즘 함수 구현\n",
    "def gradient_descent(x,y,w):\n",
    "    c = 0\n",
    "    for k in range(len(x)):\n",
    "        hx = w*x[k] \n",
    "        loss = (hx - y[k])*x[k]     # 곱하기 2를 생략한 비용함수의 미분\n",
    "        c += loss\n",
    "        # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "        # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "        # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k] = 2*x[k]*(w*x[k] - y[k])\n",
    "        # =  2*x[k]*(hx - y[k])\n",
    "    return c/len(x)\n",
    "\n",
    "# (3) 학습(fit) 함수 구현\n",
    "def fit(x,y):\n",
    "    print('----------- start learning!!')\n",
    "    w , old = 10, 100\n",
    "    for k in range(1000):\n",
    "        c = cost(x,y,w)\n",
    "        grad = gradient_descent(x,y,w)\n",
    "        w -= 0.1*grad  # 0.1:학습율(learning rate),하이퍼 파라메터,가중치의 업데이트실행\n",
    "        print('[%03d]'%k,'cost:',c,'old:',old,'weight:',w)\n",
    "    #   if c == old:  # cost의 변화가 없을 때\n",
    "        if c >= old and abs(c - old) < 1.0e-15: # cost가 1.0e-15값 보다도 더 줄지 않을 때\n",
    "            break\n",
    "        old = c\n",
    "    print('----------- end learning!!')  \n",
    "    return w\n",
    "\n",
    "# (4) 예측(predict) 함수 구현\n",
    "def predict(x,w):\n",
    "    hx = w*np.array(x)\n",
    "    return list(hx)\n",
    "\n",
    "# (5) 정확도(평가지표) 측정 함수 구현 :  정확도 검증(Valiation)  \n",
    "\n",
    "# <1> 회귀(Linear Regression) 모델 : RMSE(Root Mean Squared Error,평균 제곱근 오차)\n",
    "def get_rmse(x_test,y_test,w):\n",
    "    y_pred = predict(x_test,w)\n",
    "    print(y_pred)\n",
    "    squared_error = 0\n",
    "    for k,_ in enumerate(y_test):\n",
    "        squared_error += (y_pred[k] - y_test[k])**2  # 오차의 제곱을 합한다\n",
    "    mse = squared_error/len(y_test)   # 오차의 제곱의 평균, 평균 제곱 오차\n",
    "    rmse = np.sqrt(mse)               # 제곱근(Root), 평균 제곱근 오차\n",
    "    return rmse    \n",
    "\n",
    "# <2> 분류(classification) 모델 : 정확도(%)\n",
    "def get_accuaracy(x_test,y_test,w):\n",
    "    y_pred = predict(x_test,w)\n",
    "    print(y_pred)\n",
    "    correct = 0\n",
    "    for k,_ in enumerate(y_test) :\n",
    "        if y_test[k] == y_pred[k] :  # 실제값과 예측값이 같으면 correct를 1증가\n",
    "            correct += 1\n",
    "    accuracy = round(correct/len(y_test),2)  # 맞은 갯수/전체 갯수\n",
    "    return accuracy                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- start learning!!\n",
      "[000] cost: 704.0 old: 100 weight: 1.1999999999999993\n",
      "[001] cost: 7.040000000000012 old: 704.0 weight: 2.08\n",
      "[002] cost: 0.07040000000000013 old: 7.040000000000012 weight: 1.992\n",
      "[003] cost: 0.0007039999999999871 old: 0.07040000000000013 weight: 2.0008\n",
      "[004] cost: 7.03999999999845e-06 old: 0.0007039999999999871 weight: 1.99992\n",
      "[005] cost: 7.040000000016923e-08 old: 7.03999999999845e-06 weight: 2.0000080000000002\n",
      "[006] cost: 7.040000000546988e-10 old: 7.040000000016923e-08 weight: 1.9999992\n",
      "[007] cost: 7.040000000689097e-12 old: 7.040000000546988e-10 weight: 2.00000008\n",
      "[008] cost: 7.039999989746739e-14 old: 7.040000000689097e-12 weight: 1.999999992\n",
      "[009] cost: 7.039999978378055e-16 old: 7.039999989746739e-14 weight: 2.0000000008\n",
      "[010] cost: 7.040001164984472e-18 old: 7.039999978378055e-16 weight: 1.99999999992\n",
      "[011] cost: 7.040001164984472e-20 old: 7.040001164984472e-18 weight: 2.000000000008\n",
      "[012] cost: 7.039830636607046e-22 old: 7.040001164984472e-20 weight: 1.9999999999992\n",
      "[013] cost: 7.040612264052197e-24 old: 7.039830636607046e-22 weight: 2.00000000000008\n",
      "[014] cost: 7.028750665519215e-26 old: 7.040612264052197e-24 weight: 1.999999999999992\n",
      "[015] cost: 6.888333424389875e-28 old: 7.028750665519215e-26 weight: 2.000000000000001\n",
      "[016] cost: 7.257520328033308e-30 old: 6.888333424389875e-28 weight: 2.0\n",
      "[017] cost: 0.0 old: 7.257520328033308e-30 weight: 2.0\n",
      "[018] cost: 0.0 old: 0.0 weight: 2.0\n",
      "----------- end learning!!\n",
      "weight: 2.0\n",
      "y_pred: [12.0, 14.0, 16.0, 18.0, 20.0]\n",
      "[20.0, 40.0, 60.0]\n",
      "Accuarcy: 0.67\n",
      "[20.0, 40.0, 60.0]\n",
      "RMSE: 5.773502691896258\n"
     ]
    }
   ],
   "source": [
    "# 머신 러닝 사용자가 구현할 부분 : 모델 구현\n",
    "\n",
    "# (1) fit() 함수를 호출하여 학습 수행\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "\n",
    "w = fit(x_train,y_train)\n",
    "print('weight:',w)  # w:2.0 , cost:0.0\n",
    "\n",
    "# (2) predict() 함수를 호출하여 예측\n",
    "x_pred = [6,7,8,9,10]   # x 값만 사용\n",
    "y_pred = predict(x_pred,w)\n",
    "print('y_pred:',y_pred)  # [12.0, 14.0, 16.0, 18.0, 20.0]\n",
    "\n",
    "# (3) 정확도 측정\n",
    "# 분류 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "accuracy = get_accuaracy(x_test,y_test,w)\n",
    "print('Accuarcy:',accuracy)  # 0.67  , 절대 지표\n",
    "\n",
    "# 회귀 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "rmse = get_rmse(x_test,y_test,w)\n",
    "print('RMSE:',rmse) # RMSE: 5.773502691896258 , 상대 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class를 사용한 Linear Regression 알고리즘 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# 알고리즘 구현 : 비용함수와 경사 하강법 알고리즘 함수 구현\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self,weight=10): # 생성자\n",
    "        self.w = weight\n",
    "        print('LinearRegression 생성자')\n",
    "        \n",
    "    # (1) 비용함수 구현\n",
    "    def cost(self,x,y) :\n",
    "        c = 0\n",
    "        for k in range(len(x)):\n",
    "            hx = self.w * x[k]     # 예측 함수(방정식)\n",
    "            loss = (hx - y[k])**2  # (예측값 - 실제값)^2\n",
    "            c += loss\n",
    "        return c/len(x)            #  평균 제곱 오차\n",
    "\n",
    "    # (2) 경사 하강법 알고리즘 함수 구현\n",
    "    def gradient_descent(self,x,y):\n",
    "        c = 0\n",
    "        for k in range(len(x)):\n",
    "            hx = self.w*x[k] \n",
    "            loss = (hx - y[k])*x[k]     # 곱하기 2를 생략한 비용함수의 미분\n",
    "            c += loss\n",
    "            # 비용함수의 미분 : cost(w) = (w*x[k] - y[k])^2 의 미분\n",
    "            # cost(w) = w^2 * x[k]^2 - 2*w*x[k]*y[k] + y[k]^2\n",
    "            # cost'(w) = 2*w*x[k]^2 - 2*x[k]*y[k] = 2*x[k]*(w*x[k] - y[k])\n",
    "            # =  2*x[k]*(hx - y[k])\n",
    "        return c/len(x)\n",
    "\n",
    "    # (3) 학습(fit) 함수 구현\n",
    "    def fit(self,x,y):\n",
    "        print('----------- start learning!!')\n",
    "        old =  100\n",
    "        for k in range(1000):\n",
    "            c = self.cost(x,y)\n",
    "            grad = self.gradient_descent(x,y)\n",
    "            self.w -= 0.1*grad  # 0.1:학습율(learning rate),하이퍼 파라메터,가중치의 업데이트실행\n",
    "            print('[%03d]'%k,'cost:',c,'old:',old,'weight:',self.w)\n",
    "        #   if c == old:  # cost의 변화가 없을 때\n",
    "            if c >= old and abs(c - old) < 1.0e-15: # cost가 1.0e-15값 보다도 더 줄지 않을 때\n",
    "                break\n",
    "            old = c\n",
    "        print('----------- end learning!!')  \n",
    "#         return self.w\n",
    "\n",
    "    # (4) 예측(predict) 함수 구현\n",
    "    def predict(self,x):\n",
    "        hx = self.w*np.array(x)\n",
    "        return list(hx)\n",
    "\n",
    "    # (5) 정확도(평가지표) 측정 함수 구현 :  정확도 검증(Valiation)  \n",
    "    # <1> 분류(classification) 일 때 : 정확도(%)\n",
    "    def get_accuaracy(self,x_test,y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        print(y_pred)\n",
    "        correct = 0\n",
    "        for k,_ in enumerate(y_test) :\n",
    "            if y_test[k] == y_pred[k] :  # 실제값과 예측값이 같으면 correct를 1증가\n",
    "                correct += 1\n",
    "        accuracy = round(correct/len(y_test),2)  # 맞은 갯수/전체 갯수\n",
    "        return accuracy                \n",
    "\n",
    "    # <2> 회귀(Linear Regression) 일 때 : RMSE(Root Mean Squared Error,평균 제곱근 오차)\n",
    "    def get_rmse(self,x_test,y_test):\n",
    "        y_pred = self.predict(x_test)\n",
    "        print(y_pred)\n",
    "        squared_error = 0\n",
    "        for k,_ in enumerate(y_test):\n",
    "            squared_error += (y_pred[k] - y_test[k])**2  # 오차의 제곱을 합한다\n",
    "        mse = squared_error/len(y_test)   # 오차의 제곱의 평균, 평균 제곱 오차\n",
    "        rmse = np.sqrt(mse)               # 제곱근(Root), 평균 제곱근 오차\n",
    "        return rmse    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression 생성자\n",
      "----------- start learning!!\n",
      "[000] cost: 704.0 old: 100 weight: 1.1999999999999993\n",
      "[001] cost: 7.040000000000012 old: 704.0 weight: 2.08\n",
      "[002] cost: 0.07040000000000013 old: 7.040000000000012 weight: 1.992\n",
      "[003] cost: 0.0007039999999999871 old: 0.07040000000000013 weight: 2.0008\n",
      "[004] cost: 7.03999999999845e-06 old: 0.0007039999999999871 weight: 1.99992\n",
      "[005] cost: 7.040000000016923e-08 old: 7.03999999999845e-06 weight: 2.0000080000000002\n",
      "[006] cost: 7.040000000546988e-10 old: 7.040000000016923e-08 weight: 1.9999992\n",
      "[007] cost: 7.040000000689097e-12 old: 7.040000000546988e-10 weight: 2.00000008\n",
      "[008] cost: 7.039999989746739e-14 old: 7.040000000689097e-12 weight: 1.999999992\n",
      "[009] cost: 7.039999978378055e-16 old: 7.039999989746739e-14 weight: 2.0000000008\n",
      "[010] cost: 7.040001164984472e-18 old: 7.039999978378055e-16 weight: 1.99999999992\n",
      "[011] cost: 7.040001164984472e-20 old: 7.040001164984472e-18 weight: 2.000000000008\n",
      "[012] cost: 7.039830636607046e-22 old: 7.040001164984472e-20 weight: 1.9999999999992\n",
      "[013] cost: 7.040612264052197e-24 old: 7.039830636607046e-22 weight: 2.00000000000008\n",
      "[014] cost: 7.028750665519215e-26 old: 7.040612264052197e-24 weight: 1.999999999999992\n",
      "[015] cost: 6.888333424389875e-28 old: 7.028750665519215e-26 weight: 2.000000000000001\n",
      "[016] cost: 7.257520328033308e-30 old: 6.888333424389875e-28 weight: 2.0\n",
      "[017] cost: 0.0 old: 7.257520328033308e-30 weight: 2.0\n",
      "[018] cost: 0.0 old: 0.0 weight: 2.0\n",
      "----------- end learning!!\n",
      "weight: 2.0\n",
      "y_pred: [12.0, 14.0, 16.0, 18.0, 20.0]\n",
      "[20.0, 40.0, 60.0]\n",
      "Accuarcy: 0.67\n",
      "[20.0, 40.0, 60.0]\n",
      "RMSE: 5.773502691896258\n"
     ]
    }
   ],
   "source": [
    "# 머신 러닝 사용자가 구현할 부분 : 모델 구현 , class의 인스턴스를 생성하여 사용\n",
    "\n",
    "# (1) fit() 함수를 호출하여 학습 수행\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2,4,6,8,10]\n",
    "\n",
    "lr = LinearRegression()  # class의 인스턴스 객체를 생성\n",
    "lr.fit(x_train,y_train)  # 인스턴스 메서드를 호출\n",
    "print('weight:',lr.w)    # w:2.0 , cost:0.0\n",
    "\n",
    "# (2) predict() 함수를 호출하여 예측\n",
    "x_pred = [6,7,8,9,10]   # x 값만 사용\n",
    "y_pred = lr.predict(x_pred)  # 인스턴스 메서드를 호출\n",
    "print('y_pred:',y_pred)  # [12.0, 14.0, 16.0, 18.0, 20.0]\n",
    "\n",
    "# (3) 정확도 측정\n",
    "# 분류 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "accuracy = lr.get_accuaracy(x_test,y_test)  # 인스턴스 메서드를 호출\n",
    "print('Accuarcy:',accuracy)  # 0.67  , 절대 지표\n",
    "\n",
    "# 회귀 모델인 경우\n",
    "x_test = [10,20,30]\n",
    "# y_test = [20,40,60]\n",
    "y_test = [20,40,70]\n",
    "\n",
    "rmse = lr.get_rmse(x_test,y_test)  # 인스턴스 메서드를 호출\n",
    "print('RMSE:',rmse) # RMSE: 5.773502691896258 , 상대 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
